# -*- coding: utf-8 -*-
"""INS_Innovative.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XH8c0fCoDcJ2cj-LFz20f_Wus7w4HerK

# **SIMPLE IMPLEMENTATION**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

def create_subplot_figure(num_rows, num_cols, subplot_titles, height=800):
    """Helper function to create subplot figure with consistent styling"""
    fig = make_subplots(
        rows=num_rows,
        cols=num_cols,
        subplot_titles=subplot_titles,
        vertical_spacing=0.15,
        horizontal_spacing=0.1
    )
    fig.update_layout(
        height=height,
        showlegend=True,
        template='plotly_white',
        title_x=0.5,
        font=dict(size=10)
    )
    return fig

def prepare_data(combined_df):
    """Prepare data for modeling"""
    # Calculate additional features
    numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

    # Standardize features
    scaler = StandardScaler()
    X = scaler.fit_transform(combined_df[numeric_cols])

    return X, numeric_cols


def create_3d_traffic_plots(combined_df):
    """Create interactive 3D visualizations of network traffic patterns"""
    try:
        # 1. 3D Scatter Plot: Bytes vs Packets vs Duration
        fig_3d_scatter = go.Figure(data=[go.Scatter3d(
            x=np.log10(combined_df['fwd_num_bytes'] + 1),
            y=np.log10(combined_df['fwd_num_pkts'] + 1),
            z=np.log10(combined_df['flow_duration'] + 1),
            mode='markers',
            marker=dict(
                size=5,
                color=combined_df['fwd_mean_iat'],
                colorscale='Viridis',
                opacity=0.8,
                colorbar=dict(title='Mean IAT')
            ),
            hovertemplate=
            '<b>Forward Bytes</b>: %{x:.2f}<br>' +
            '<b>Forward Packets</b>: %{y:.2f}<br>' +
            '<b>Flow Duration</b>: %{z:.2f}<br>' +
            '<b>Mean IAT</b>: %{marker.color:.2f}<extra></extra>'
        )])

        fig_3d_scatter.update_layout(
            title='3D Network Traffic Pattern Analysis',
            scene=dict(
                xaxis_title='Log10(Forward Bytes)',
                yaxis_title='Log10(Forward Packets)',
                zaxis_title='Log10(Flow Duration)',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_scatter.show()

        # 2. 3D Surface Plot: Traffic Volume Over Time
        x_bins = np.linspace(0, np.log10(combined_df['fwd_num_bytes'].max() + 1), 50)
        y_bins = np.linspace(0, np.log10(combined_df['flow_duration'].max() + 1), 50)

        hist, x_edges, y_edges = np.histogram2d(
            np.log10(combined_df['fwd_num_bytes'] + 1),
            np.log10(combined_df['flow_duration'] + 1),
            bins=[x_bins, y_bins]
        )

        fig_3d_surface = go.Figure(data=[go.Surface(
            x=x_edges[:-1],
            y=y_edges[:-1],
            z=hist,
            colorscale='Viridis',
            hoverinfo='x+y+z',  # Replaced hoverongaps with proper hover configuration
            hovertemplate=
            '<b>Log10(Bytes)</b>: %{x:.2f}<br>' +
            '<b>Log10(Duration)</b>: %{y:.2f}<br>' +
            '<b>Count</b>: %{z}<extra></extra>'
        )])

        fig_3d_surface.update_layout(
            title='3D Traffic Volume Distribution',
            scene=dict(
                xaxis_title='Log10(Forward Bytes)',
                yaxis_title='Log10(Flow Duration)',
                zaxis_title='Traffic Density',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_surface.show()

        # 3. 3D Network Protocol Analysis
        protocol_metrics = combined_df.groupby('proto').agg({
            'fwd_num_bytes': 'mean',
            'fwd_num_pkts': 'mean',
            'flow_duration': 'mean'
        }).reset_index()

        fig_3d_bar = go.Figure(data=[go.Scatter3d(
            x=protocol_metrics['fwd_num_bytes'],
            y=protocol_metrics['fwd_num_pkts'],
            z=protocol_metrics['flow_duration'],
            mode='markers+text',
            marker=dict(
                size=20,
                color=range(len(protocol_metrics)),
                colorscale='Viridis',
                opacity=0.8
            ),
            text=protocol_metrics['proto'],
            textposition='top center',
            hovertemplate=
            '<b>Protocol</b>: %{text}<br>' +
            '<b>Avg Bytes</b>: %{x:.2f}<br>' +
            '<b>Avg Packets</b>: %{y:.2f}<br>' +
            '<b>Avg Duration</b>: %{z:.2f}<extra></extra>'
        )])

        fig_3d_bar.update_layout(
            title='3D Protocol Characteristics Analysis',
            scene=dict(
                xaxis_title='Average Forward Bytes',
                yaxis_title='Average Forward Packets',
                zaxis_title='Average Flow Duration',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_bar.show()

    except Exception as e:
        print(f"Error in 3D visualization: {e}")


def train_anomaly_detectors(X):
    """Train multiple anomaly detection models"""
    # Isolation Forest
    iso_forest = IsolationForest(contamination=0.1, random_state=42)
    iso_forest_scores = iso_forest.fit_predict(X)

    # One-Class SVM
    ocsvm = OneClassSVM(kernel='rbf', nu=0.1)
    ocsvm_scores = ocsvm.fit_predict(X)

    return iso_forest_scores, ocsvm_scores

def analyze_network_traffic(file_paths):
    # Load and combine data
    dataframes = []
    for path in file_paths:
        try:
            df = pd.read_csv(path)
            dataframes.append(df)
        except Exception as e:
            print(f"Error loading {path}: {e}")

    if not dataframes:
        print("No data loaded. Please check file paths.")
        return

    combined_df = pd.concat(dataframes, ignore_index=True)

    # Calculate flow duration
    combined_df['flow_duration'] = combined_df['fwd_mean_iat'] * combined_df['fwd_num_pkts']

    # 1. Protocol Distribution
    proto_counts = combined_df['proto'].value_counts()
    fig_proto = go.Figure(data=[go.Pie(
        labels=proto_counts.index,
        values=proto_counts.values,
        hole=0.3
    )])
    fig_proto.update_layout(title='Protocol Distribution', height=500)
    fig_proto.show()

    # 2. Basic Flow Metrics
    fig_basic = create_subplot_figure(2, 2, [
        'Bytes Distribution',
        'Packets Distribution',
        'Flow Duration Distribution',
        'IAT Distribution'
    ])

    fig_basic.add_trace(
        go.Histogram(x=np.log10(combined_df['fwd_num_bytes'] + 1),
                    name='Forward Bytes'),
        row=1, col=1
    )
    fig_basic.add_trace(
        go.Histogram(x=np.log10(combined_df['fwd_num_pkts'] + 1),
                    name='Forward Packets'),
        row=1, col=2
    )
    fig_basic.add_trace(
        go.Histogram(x=np.log10(combined_df['flow_duration'] + 1),
                    name='Flow Duration'),
        row=2, col=1
    )
    fig_basic.add_trace(
        go.Histogram(x=np.log10(combined_df['fwd_mean_iat'] + 1),
                    name='Forward IAT'),
        row=2, col=2
    )
    fig_basic.show()

    # 3. Advanced Flow Analysis
    fig_advanced = create_subplot_figure(2, 2, [
        'Bytes vs Packets',
        'Protocol-wise Flow Duration',
        'IAT vs Packet Length',
        'Flow Size Distribution by Protocol'
    ])

    fig_advanced.add_trace(
        go.Scatter(
            x=combined_df['fwd_num_pkts'],
            y=combined_df['fwd_num_bytes'],
            mode='markers',
            marker=dict(
                color=combined_df['proto'].astype('category').cat.codes,
                colorscale='Viridis'
            ),
            name='Bytes vs Packets'
        ),
        row=1, col=1
    )

    fig_advanced.add_trace(
        go.Box(
            x=combined_df['proto'],
            y=np.log10(combined_df['flow_duration'] + 1),
            name='Duration by Protocol'
        ),
        row=1, col=2
    )

    fig_advanced.add_trace(
        go.Scatter(
            x=combined_df['fwd_mean_iat'],
            y=combined_df['fwd_mean_pkt_len'],
            mode='markers',
            marker=dict(
                color=combined_df['proto'].astype('category').cat.codes,
                colorscale='Viridis'
            ),
            name='IAT vs Packet Length'
        ),
        row=2, col=1
    )

    fig_advanced.add_trace(
        go.Box(
            x=combined_df['proto'],
            y=np.log10(combined_df['fwd_num_bytes'] + combined_df['bwd_num_bytes'] + 1),
            name='Flow Size by Protocol'
        ),
        row=2, col=2
    )
    fig_advanced.show()

    # 4. Anomaly Detection Analysis
    X, numeric_cols = prepare_data(combined_df)
    iso_forest_scores, ocsvm_scores = train_anomaly_detectors(X)

    combined_df['iso_forest_anomaly'] = iso_forest_scores
    combined_df['ocsvm_anomaly'] = ocsvm_scores

    fig_anomaly = create_subplot_figure(2, 2, [
        'Isolation Forest Anomalies',
        'One-Class SVM Anomalies',
        'Anomaly Score Distribution',
        'Anomaly Timeline'
    ])

    # Isolation Forest Anomalies
    fig_anomaly.add_trace(
        go.Scatter(
            x=combined_df['fwd_num_pkts'],
            y=combined_df['fwd_num_bytes'],
            mode='markers',
            marker=dict(
                color=iso_forest_scores,
                colorscale='RdYlBu'
            ),
            name='Isolation Forest'
        ),
        row=1, col=1
    )

    # One-Class SVM Anomalies
    fig_anomaly.add_trace(
        go.Scatter(
            x=combined_df['fwd_num_pkts'],
            y=combined_df['fwd_num_bytes'],
            mode='markers',
            marker=dict(
                color=ocsvm_scores,
                colorscale='RdYlBu'
            ),
            name='One-Class SVM'
        ),
        row=1, col=2
    )

    # Anomaly Score Distribution
    fig_anomaly.add_trace(
        go.Histogram(x=iso_forest_scores, name='Isolation Forest Scores'),
        row=2, col=1
    )

    # Anomaly Timeline
    fig_anomaly.add_trace(
        go.Scatter(
            y=pd.Series(iso_forest_scores).rolling(window=50).mean(),
            name='Anomaly Timeline'
        ),
        row=2, col=2
    )
    fig_anomaly.show()

    # 5. Feature Importance Analysis
    # Train a Random Forest for feature importance
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    y = (iso_forest_scores == -1).astype(int)  # Convert anomaly scores to binary labels
    rf.fit(X, y)

    feature_importance = pd.DataFrame({
        'feature': numeric_cols,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)

    fig_importance = go.Figure(data=[
        go.Bar(
            x=feature_importance['feature'][:15],  # Top 15 features
            y=feature_importance['importance'][:15]
        )
    ])
    fig_importance.update_layout(
        title='Top 15 Feature Importance',
        xaxis_title='Features',
        yaxis_title='Importance Score',
        height=500
    )
    fig_importance.show()

    # 6. IAT Analysis
    fig_iat = create_subplot_figure(2, 2, [
        'Forward vs Backward IAT',
        'IAT Distribution by Protocol',
        'IAT Time Series',
        'IAT vs Flow Duration'
    ])

    fig_iat.add_trace(
        go.Scatter(
            x=combined_df['fwd_mean_iat'],
            y=combined_df['bwd_mean_iat'],
            mode='markers',
            marker=dict(
                color=combined_df['proto'].astype('category').cat.codes,
                colorscale='Viridis'
            ),
            name='IAT Comparison'
        ),
        row=1, col=1
    )

    fig_iat.add_trace(
        go.Box(
            x=combined_df['proto'],
            y=np.log10(combined_df['fwd_mean_iat'] + 1),
            name='Forward IAT by Protocol'
        ),
        row=1, col=2
    )

    fig_iat.add_trace(
        go.Scatter(
            y=combined_df['fwd_mean_iat'].rolling(window=50).mean(),
            name='IAT Moving Average'
        ),
        row=2, col=1
    )

    fig_iat.add_trace(
        go.Scatter(
            x=combined_df['flow_duration'],
            y=combined_df['fwd_mean_iat'],
            mode='markers',
            marker=dict(
                color=combined_df['proto'].astype('category').cat.codes,
                colorscale='Viridis'
            ),
            name='IAT vs Duration'
        ),
        row=2, col=2
    )
    fig_iat.show()

    # 7. Detection Timeline and Model Performance
    # Split data for basic detection model
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Train a basic model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    fig_model = create_subplot_figure(2, 2, [
        'Detection Timeline',
        'ROC Curve',
        'Confusion Matrix',
        'Prediction Distribution'
    ])

    # Detection Timeline
    fig_model.add_trace(
        go.Scatter(
            y=pd.Series(y_prob).rolling(window=50).mean(),
            name='Detection Score'
        ),
        row=1, col=1
    )

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    fig_model.add_trace(
        go.Scatter(x=fpr, y=tpr, name=f'ROC (AUC = {auc(fpr, tpr):.3f})'),
        row=1, col=2
    )

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    fig_model.add_trace(
        go.Heatmap(
            z=cm,
            x=['Normal', 'Anomaly'],
            y=['Normal', 'Anomaly'],
            colorscale='RdBu'
        ),
        row=2, col=1
    )

    # Prediction Distribution
    fig_model.add_trace(
        go.Histogram(x=y_prob, name='Prediction Scores'),
        row=2, col=2
    )
    fig_model.show()

def main():
    file_paths = [
        '/content/new_biflow_Thursday-WorkingHours_Wev_XSS.csv',
        '/content/new_biflow_Thursday-WorkingHours_CoolDisk.csv',
        '/content/new_biflow_Thursday-WorkingHours_Dropbox1.csv',
        '/content/new_biflow_Thursday-WorkingHours_Dropbox2.csv',
        '/content/new_biflow_Thursday-WorkingHours_Web_BF.csv'
    ]

    analyze_network_traffic(file_paths)

if __name__ == "__main__":
    main()

"""# **WITH RL IMPLEMENTATION**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = []
        self.gamma = 0.95    # discount rate
        self.epsilon = 1.0   # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential([
            Dense(64, input_dim=self.state_size, activation='relu'),
            Dropout(0.2),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(self.action_size, activation='linear')
        ])
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.randint(self.action_size)
        act_values = self.model.predict(state, verbose=0)
        return np.argmax(act_values[0])

    def train(self, state, action, reward, next_state, done):
        target = reward
        if not done:
            target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])
        target_f = self.model.predict(state, verbose=0)
        target_f[0][action] = target
        self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

def create_subplot_figure(num_rows, num_cols, subplot_titles, height=1000, width=1200):
    """Enhanced helper function to create subplot figure with improved styling"""
    try:
        fig = make_subplots(
            rows=num_rows,
            cols=num_cols,
            subplot_titles=subplot_titles,
            vertical_spacing=0.2,
            horizontal_spacing=0.15
        )
        fig.update_layout(
            height=height,
            width=width,
            showlegend=True,
            template='plotly_white',
            title_x=0.5,
            font=dict(size=12),
            title_font=dict(size=16),
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="right",
                x=0.99,
                bgcolor="rgba(255, 255, 255, 0.8)"
            )
        )
        return fig
    except Exception as e:
        print(f"Error creating subplot figure: {e}")
        return None

def prepare_data(combined_df):
    """Enhanced data preparation with feature engineering"""
    try:
        # Calculate additional features
        numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

        # Add new features for RL
        combined_df['bytes_per_packet'] = (combined_df['fwd_num_bytes'] + combined_df['bwd_num_bytes']) / \
                                        (combined_df['fwd_num_pkts'] + combined_df['bwd_num_pkts']).replace(0, 1)
        combined_df['packet_rate'] = (combined_df['fwd_num_pkts'] + combined_df['bwd_num_pkts']) / \
                                    combined_df['flow_duration'].replace(0, 1)

        # Update numeric_cols to include new features
        numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

        # Standardize features
        scaler = StandardScaler()
        X = scaler.fit_transform(combined_df[numeric_cols])

        return X, numeric_cols
    except Exception as e:
        print(f"Error in data preparation: {e}")
        return None, None

def train_anomaly_detectors(X):
    """Train multiple anomaly detection models"""
    try:
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        iso_forest_scores = iso_forest.fit_predict(X)

        # One-Class SVM
        ocsvm = OneClassSVM(kernel='rbf', nu=0.1)
        ocsvm_scores = ocsvm.fit_predict(X)

        return iso_forest_scores, ocsvm_scores
    except Exception as e:
        print(f"Error in training anomaly detectors: {e}")
        return None, None

def train_rl_detector(X_train, y_train, epochs=10):
    """Train RL-based anomaly detector"""
    try:
        state_size = X_train.shape[1]
        action_size = 2  # Normal or Anomaly
        agent = DQNAgent(state_size, action_size)

        batch_size = 32
        rewards_history = []

        for epoch in range(epochs):
            total_reward = 0
            for i in range(0, len(X_train), batch_size):
                state = X_train[i:i+batch_size]
                if len(state) < batch_size:
                    break

                state = np.reshape(state[0], [1, state_size])
                action = agent.act(state)

                # Calculate reward based on correct prediction
                true_label = y_train[i]
                reward = 1 if action == true_label else -1

                next_state = X_train[i+1:i+batch_size+1]
                if len(next_state) < batch_size:
                    break
                next_state = np.reshape(next_state[0], [1, state_size])

                done = i + batch_size >= len(X_train)
                agent.train(state, action, reward, next_state, done)
                total_reward += reward

            rewards_history.append(total_reward)
            if epoch % 10 == 0:
                print(f"Epoch: {epoch}, Total Reward: {total_reward}")

        return agent, rewards_history
    except Exception as e:
        print(f"Error in training RL detector: {e}")
        return None, None

def analyze_network_traffic(file_paths):
    """Main analysis function with error handling"""
    try:
        # Load and combine data
        dataframes = []
        for path in file_paths:
            try:
                df = pd.read_csv(path)
                dataframes.append(df)
            except Exception as e:
                print(f"Error loading {path}: {e}")
                continue

        if not dataframes:
            print("No data loaded. Please check file paths.")
            return

        combined_df = pd.concat(dataframes, ignore_index=True)

        # Calculate flow duration
        combined_df['flow_duration'] = combined_df['fwd_mean_iat'] * combined_df['fwd_num_pkts']
        combined_df['flow_duration'] = combined_df['flow_duration'].replace(0, 1)  # Avoid division by zero

        # 1. Enhanced Protocol Distribution
        try:
            # proto_counts = combined_df['proto'].value_counts()
            # fig_proto = go.Figure(data=[go.Pie(
            #     labels=proto_counts.index,
            #     values=proto_counts.values,
            #     hole=0.3,
            #     textinfo='label+percent',
            #     marker=dict(colors=px.colors.qualitative.Set3)
            # )])
            # fig_proto.update_layout(
            #     title='Network Protocol Distribution Analysis',
            #     annotations=[dict(text="Protocol Types", x=0.5, y=0.5, font_size=14, showarrow=False)],
            #     height=600,
            #     width=800
            # )
            # fig_proto.show()
            protocol_map = {
                6: '6: TCP',
                17: '17: UDP'
            }

            # Replace the numbers in the 'proto' column with their mapped labels
            combined_df['proto'] = combined_df['proto'].map(protocol_map).fillna(combined_df['proto'])

            # Create the pie chart
            fig_proto = go.Figure(data=[go.Pie(
                labels=combined_df['proto'].value_counts().index,
                values=combined_df['proto'].value_counts().values,
                textinfo='label+percent',
                marker=dict(colors=px.colors.qualitative.Set3),
                hole=0.3
            )])

            # Update layout for visualization
            fig_proto.update_layout(
                title='Network Protocol Distribution Analysis',
                annotations=[dict(text="Protocol Types", x=0.5, y=0.5, font_size=14, showarrow=False)],
                height=600,
                width=800
            )

            # fig_proto.update_layout(title='Protocol Distribution', height=500)
            fig_proto.show()
        except Exception as e:
            print(f"Error in protocol distribution visualization: {e}")

        # 2. Enhanced Flow Metrics
        try:
            fig_basic = create_subplot_figure(2, 2, [
                'Network Bytes Distribution (log scale)',
                'Packet Count Distribution (log scale)',
                'Flow Duration Analysis (log scale)',
                'Inter-Arrival Time Distribution (log scale)'
            ])

            if fig_basic:
                for i, (data, name) in enumerate([
                    (np.log10(combined_df['fwd_num_bytes'] + 1), 'Forward Bytes'),
                    (np.log10(combined_df['fwd_num_pkts'] + 1), 'Forward Packets'),
                    (np.log10(combined_df['flow_duration'] + 1), 'Flow Duration'),
                    (np.log10(combined_df['fwd_mean_iat'] + 1), 'Forward IAT')
                ]):
                    fig_basic.add_trace(
                        go.Histogram(
                            x=data,
                            name=name,
                            nbinsx=50,
                            marker_color=px.colors.qualitative.Set2[i]
                        ),
                        row=(i // 2) + 1,
                        col=(i % 2) + 1
                    )
                    fig_basic.update_xaxes(title_text='Log10 Scale', row=(i // 2) + 1, col=(i % 2) + 1)
                    fig_basic.update_yaxes(title_text='Count', row=(i // 2) + 1, col=(i % 2) + 1)

                fig_basic.show()
        except Exception as e:
            print(f"Error in flow metrics visualization: {e}")

        # Prepare data for modeling
        X, numeric_cols = prepare_data(combined_df)
        if X is None or numeric_cols is None:
            return

        # Train traditional anomaly detectors
        iso_forest_scores, ocsvm_scores = train_anomaly_detectors(X)
        if iso_forest_scores is None or ocsvm_scores is None:
            return

        # Prepare data for RL
        y = (iso_forest_scores == -1).astype(int)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train RL detector
        rl_agent, rewards_history = train_rl_detector(X_train, y_train, epochs=30)
        if rl_agent is None or rewards_history is None:
            return

        # Generate RL predictions
        rl_predictions = []
        try:
            for i in range(len(X_test)):
                state = np.reshape(X_test[i], [1, X_test.shape[1]])
                rl_predictions.append(rl_agent.act(state))
        except Exception as e:
            print(f"Error generating RL predictions: {e}")
            return

        # 3. Model Comparison Analysis
        try:
            fig_comparison = create_subplot_figure(2, 2, [
                'Detection Methods Comparison',
                'RL Training Rewards',
                'Model Performance Metrics',
                'ROC Curves Comparison'
            ])

            if fig_comparison:
                # Detection Methods Comparison
                methods = ['Isolation Forest', 'One-Class SVM', 'RL-Based']
                scores = [
                    accuracy_score(y_test, iso_forest_scores[-len(y_test):] == -1),
                    accuracy_score(y_test, ocsvm_scores[-len(y_test):] == -1),
                    accuracy_score(y_test, rl_predictions)
                ]

                fig_comparison.add_trace(
                    go.Bar(
                        x=methods,
                        y=scores,
                        marker_color=px.colors.qualitative.Set1
                    ),
                    row=1, col=1
                )

                # RL Training Rewards
                fig_comparison.add_trace(
                    go.Scatter(
                        y=rewards_history,
                        mode='lines',
                        name='RL Rewards',
                        line=dict(color=px.colors.qualitative.Set1[2])
                    ),
                    row=1, col=2
                )

                # Model Performance Metrics
                metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
                rl_metrics = [
                    accuracy_score(y_test, rl_predictions),
                    precision_score(y_test, rl_predictions),
                    recall_score(y_test, rl_predictions),
                    f1_score(y_test, rl_predictions)
                ]

                fig_comparison.add_trace(
                    go.Bar(
                        x=metrics,
                        y=rl_metrics,
                        name='RL Model Metrics',
                        marker_color=px.colors.qualitative.Set1[2]
                    ),
                    row=2, col=1
                )

                # ROC Curves
                for model_name, y_pred in [
                    ('Isolation Forest', iso_forest_scores[-len(y_test):] == -1),
                    ('One-Class SVM', ocsvm_scores[-len(y_test):] == -1),
                    ('RL-Based', rl_predictions)
                ]:
                    fpr, tpr, _ = roc_curve(y_test, y_pred)
                    auc_score = auc(fpr, tpr)
                    fig_comparison.add_trace(
                        go.Scatter(
                            x=fpr,
                            y=tpr,
                            name=f'{model_name} (AUC={auc_score:.3f})',
                            mode='lines'
                        ),
                        row=2, col=2
                    )

                fig_comparison.update_xaxes(title_text='False Positive Rate', row=2, col=2)
                fig_comparison.update_yaxes(title_text='True Positive Rate', row=2, col=2)
                fig_comparison.show()
        except Exception as e:
            print(f"Error in model comparison visualization: {e}")

    except Exception as e:
        print(f"Error in network traffic analysis: {e}")

def main():
    """Main function with error handling"""
    try:
        file_paths = [
            '/content/new_biflow_Thursday-WorkingHours_Wev_XSS.csv',
            '/content/new_biflow_Thursday-WorkingHours_CoolDisk.csv',
            '/content/new_biflow_Thursday-WorkingHours_Dropbox1.csv',
            '/content/new_biflow_Thursday-WorkingHours_Dropbox2.csv',
            '/content/new_biflow_Thursday-WorkingHours_Web_BF.csv'
        ]

        print("Starting network traffic analysis...")
        analyze_network_traffic(file_paths)
        print("Analysis completed successfully!")

    except Exception as e:
        print(f"Error in main function: {e}")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Helper function for subplots with styling
def create_subplot_figure(num_rows, num_cols, subplot_titles, height=800):
    fig = make_subplots(
        rows=num_rows,
        cols=num_cols,
        subplot_titles=subplot_titles,
        vertical_spacing=0.15,
        horizontal_spacing=0.1
    )
    fig.update_layout(
        height=height,
        showlegend=True,
        template='plotly_white',
        title_x=0.5,
        font=dict(size=10)
    )
    return fig

# Prepares data for training
def prepare_data(combined_df):
    numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns
    scaler = StandardScaler()
    X = scaler.fit_transform(combined_df[numeric_cols])
    return X, numeric_cols

# RL-based Anomaly Detector
def train_rl_anomaly_detector(X):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X.shape[1],)),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())
    y_labels = np.zeros(len(X))  # Normal data assumed initially
    model.fit(X, y_labels, epochs=10, batch_size=32, verbose=1)
    scores = model.predict(X)
    return scores

# Main analysis function
def analyze_network_traffic(file_paths):
    dataframes = []
    for path in file_paths:
        try:
            df = pd.read_csv(path)
            dataframes.append(df)
        except Exception as e:
            print(f"Error loading {path}: {e}")

    if not dataframes:
        print("No data loaded. Please check file paths.")
        return

    combined_df = pd.concat(dataframes, ignore_index=True)
    combined_df['flow_duration'] = combined_df['fwd_mean_iat'] * combined_df['fwd_num_pkts']
    X, numeric_cols = prepare_data(combined_df)

    # Train Anomaly Detectors
    iso_forest = IsolationForest(contamination=0.1, random_state=42)
    iso_forest_scores = iso_forest.fit_predict(X)

    ocsvm = OneClassSVM(kernel='rbf', nu=0.1)
    ocsvm_scores = ocsvm.fit_predict(X)

    # RL-based Detection
    rl_scores = train_rl_anomaly_detector(X)
    combined_df['iso_forest_anomaly'] = iso_forest_scores
    combined_df['ocsvm_anomaly'] = ocsvm_scores
    combined_df['rl_anomaly_score'] = rl_scores


    protocol_map = {
        6: '6: TCP',
        17: '17: UDP'
    }

    # Replace the numbers in the 'proto' column with their mapped labels
    combined_df['proto'] = combined_df['proto'].map(protocol_map).fillna(combined_df['proto'])

    # Create the pie chart
    fig_proto = go.Figure(data=[go.Pie(
        labels=combined_df['proto'].value_counts().index,
        values=combined_df['proto'].value_counts().values,
        hole=0.3
    )])

    # Update layout for visualization
    fig_proto.update_layout(
        title='Network Protocol Distribution Analysis',
        annotations=[dict(text="Protocol Types", x=0.5, y=0.5, font_size=14, showarrow=False)],
        height=600,
        width=800
    )

    # Visualization and output formatting
    fig_proto = go.Figure(data=[go.Pie(
        labels=combined_df['proto'].value_counts().index,
        values=combined_df['proto'].value_counts().values,
        hole=0.3
    )])
    # fig_proto.update_layout(title='Protocol Distribution', height=500)
    fig_proto.show()

    fig_basic = create_subplot_figure(2, 2, [
        'Forward Bytes Distribution',
        'Forward Packets Distribution',
        'Flow Duration Distribution',
        'Forward IAT Distribution'
    ])
    fig_basic.add_trace(go.Histogram(x=np.log10(combined_df['fwd_num_bytes'] + 1), name='Forward Bytes'), row=1, col=1)
    fig_basic.add_trace(go.Histogram(x=np.log10(combined_df['fwd_num_pkts'] + 1), name='Forward Packets'), row=1, col=2)
    fig_basic.add_trace(go.Histogram(x=np.log10(combined_df['flow_duration'] + 1), name='Flow Duration'), row=2, col=1)
    fig_basic.add_trace(go.Histogram(x=np.log10(combined_df['fwd_mean_iat'] + 1), name='Forward IAT'), row=2, col=2)
    fig_basic.show()

    fig_anomaly = create_subplot_figure(2, 2, [
        'Isolation Forest Anomalies',
        'One-Class SVM Anomalies',
        'RL-based Anomaly Scores',
        'Anomaly Timeline'
    ])
    fig_anomaly.add_trace(go.Scatter(
        x=combined_df['fwd_num_pkts'],
        y=combined_df['fwd_num_bytes'],
        mode='markers',
        marker=dict(color=iso_forest_scores, colorscale='RdYlBu'),
        name='Isolation Forest'
    ), row=1, col=1)
    fig_anomaly.add_trace(go.Scatter(
        x=combined_df['fwd_num_pkts'],
        y=combined_df['fwd_num_bytes'],
        mode='markers',
        marker=dict(color=ocsvm_scores, colorscale='RdYlBu'),
        name='One-Class SVM'
    ), row=1, col=2)
    fig_anomaly.add_trace(go.Scatter(
        x=combined_df['fwd_num_pkts'],
        y=combined_df['fwd_num_bytes'],
        mode='markers',
        marker=dict(color=combined_df['rl_anomaly_score'], colorscale='Viridis'),
        name='RL Anomaly Scores'
    ), row=2, col=1)
    fig_anomaly.add_trace(go.Scatter(
        y=pd.Series(combined_df['rl_anomaly_score']).rolling(window=50).mean(),
        name='Anomaly Timeline (RL)'
    ), row=2, col=2)
    fig_anomaly.show()

    # Feature Importance
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    y = (iso_forest_scores == -1).astype(int)
    rf.fit(X, y)
    feature_importance = pd.DataFrame({
        'feature': numeric_cols,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)

    fig_importance = go.Figure(data=[
        go.Bar(
            x=feature_importance['feature'][:15],
            y=feature_importance['importance'][:15]
        )
    ])
    fig_importance.update_layout(
        title='Top 15 Feature Importance',
        xaxis_title='Features',
        yaxis_title='Importance Score',
        height=500
    )
    fig_importance.show()

    # ROC Curve for RL Model
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    rl_model = Sequential([
        Dense(128, activation='relu', input_shape=(X.shape[1],)),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    rl_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')
    rl_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    y_prob = rl_model.predict(X_test)
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    auc_score = auc(fpr, tpr)

    fig_roc = go.Figure(go.Scatter(x=fpr, y=tpr, name=f'ROC Curve (AUC = {auc_score:.3f})'))
    fig_roc.update_layout(
        title='ROC Curve for RL-based IDS',
        xaxis_title='False Positive Rate',
        yaxis_title='True Positive Rate',
        height=500
    )
    fig_roc.show()

def main():
    file_paths = [
        '/content/new_biflow_Thursday-WorkingHours_Wev_XSS.csv',
        '/content/new_biflow_Thursday-WorkingHours_CoolDisk.csv',
        '/content/new_biflow_Thursday-WorkingHours_Dropbox1.csv',
        '/content/new_biflow_Thursday-WorkingHours_Dropbox2.csv',
        '/content/new_biflow_Thursday-WorkingHours_Web_BF.csv'
    ]
    analyze_network_traffic(file_paths)

if __name__ == "__main__":
    main()

"""demo
# **new**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = []
        self.gamma = 0.95    # discount rate
        self.epsilon = 1.0   # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential([
            Dense(64, input_dim=self.state_size, activation='relu'),
            Dropout(0.2),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(self.action_size, activation='linear')
        ])
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.randint(self.action_size)
        act_values = self.model.predict(state, verbose=0)
        return np.argmax(act_values[0])

    def train(self, state, action, reward, next_state, done):
        target = reward
        if not done:
            target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])
        target_f = self.model.predict(state, verbose=0)
        target_f[0][action] = target
        self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

def create_subplot_figure(num_rows, num_cols, subplot_titles, height=1000, width=1200):
    """Enhanced helper function to create subplot figure with improved styling"""
    try:
        fig = make_subplots(
            rows=num_rows,
            cols=num_cols,
            subplot_titles=subplot_titles,
            vertical_spacing=0.2,
            horizontal_spacing=0.15
        )
        fig.update_layout(
            height=height,
            width=width,
            showlegend=True,
            template='plotly_white',
            title_x=0.5,
            font=dict(size=12),
            title_font=dict(size=16),
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="right",
                x=0.99,
                bgcolor="rgba(255, 255, 255, 0.8)"
            )
        )
        return fig
    except Exception as e:
        print(f"Error creating subplot figure: {e}")
        return None

def prepare_data(combined_df):
    """Enhanced data preparation with feature engineering"""
    try:
        # Calculate additional features
        numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

        # Add new features for RL
        combined_df['bytes_per_packet'] = (combined_df['fwd_num_bytes'] + combined_df['bwd_num_bytes']) / \
                                        (combined_df['fwd_num_pkts'] + combined_df['bwd_num_pkts']).replace(0, 1)
        combined_df['packet_rate'] = (combined_df['fwd_num_pkts'] + combined_df['bwd_num_pkts']) / \
                                    combined_df['flow_duration'].replace(0, 1)

        # Update numeric_cols to include new features
        numeric_cols = combined_df.select_dtypes(include=['float64', 'int64']).columns

        # Standardize features
        scaler = StandardScaler()
        X = scaler.fit_transform(combined_df[numeric_cols])

        return X, numeric_cols
    except Exception as e:
        print(f"Error in data preparation: {e}")
        return None, None

def create_3d_traffic_plots(combined_df):
    """Create interactive 3D visualizations of network traffic patterns"""
    try:
        # 1. 3D Scatter Plot: Bytes vs Packets vs Duration
        fig_3d_scatter = go.Figure(data=[go.Scatter3d(
            x=np.log10(combined_df['fwd_num_bytes'] + 1),
            y=np.log10(combined_df['fwd_num_pkts'] + 1),
            z=np.log10(combined_df['flow_duration'] + 1),
            mode='markers',
            marker=dict(
                size=5,
                color=combined_df['fwd_mean_iat'],
                colorscale='Viridis',
                opacity=0.8,
                colorbar=dict(title='Mean IAT')
            ),
            hovertemplate=
            '<b>Forward Bytes</b>: %{x:.2f}<br>' +
            '<b>Forward Packets</b>: %{y:.2f}<br>' +
            '<b>Flow Duration</b>: %{z:.2f}<br>' +
            '<b>Mean IAT</b>: %{marker.color:.2f}<extra></extra>'
        )])

        fig_3d_scatter.update_layout(
            title='3D Network Traffic Pattern Analysis',
            scene=dict(
                xaxis_title='Log10(Forward Bytes)',
                yaxis_title='Log10(Forward Packets)',
                zaxis_title='Log10(Flow Duration)',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_scatter.show()

        # 2. 3D Surface Plot: Traffic Volume Over Time
        x_bins = np.linspace(0, np.log10(combined_df['fwd_num_bytes'].max() + 1), 50)
        y_bins = np.linspace(0, np.log10(combined_df['flow_duration'].max() + 1), 50)

        hist, x_edges, y_edges = np.histogram2d(
            np.log10(combined_df['fwd_num_bytes'] + 1),
            np.log10(combined_df['flow_duration'] + 1),
            bins=[x_bins, y_bins]
        )

        fig_3d_surface = go.Figure(data=[go.Surface(
            x=x_edges[:-1],
            y=y_edges[:-1],
            z=hist,
            colorscale='Viridis',
            hoverongaps=False,
            hovertemplate=
            '<b>Log10(Bytes)</b>: %{x:.2f}<br>' +
            '<b>Log10(Duration)</b>: %{y:.2f}<br>' +
            '<b>Count</b>: %{z}<extra></extra>'
        )])

        fig_3d_surface.update_layout(
            title='3D Traffic Volume Distribution',
            scene=dict(
                xaxis_title='Log10(Forward Bytes)',
                yaxis_title='Log10(Flow Duration)',
                zaxis_title='Traffic Density',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_surface.show()

        # 3. 3D Network Protocol Analysis
        protocol_metrics = combined_df.groupby('proto').agg({
            'fwd_num_bytes': 'mean',
            'fwd_num_pkts': 'mean',
            'flow_duration': 'mean'
        }).reset_index()

        fig_3d_bar = go.Figure(data=[go.Scatter3d(
            x=protocol_metrics['fwd_num_bytes'],
            y=protocol_metrics['fwd_num_pkts'],
            z=protocol_metrics['flow_duration'],
            mode='markers+text',
            marker=dict(
                size=20,
                color=range(len(protocol_metrics)),
                colorscale='Viridis',
                opacity=0.8
            ),
            text=protocol_metrics['proto'],
            textposition='top center',
            hovertemplate=
            '<b>Protocol</b>: %{text}<br>' +
            '<b>Avg Bytes</b>: %{x:.2f}<br>' +
            '<b>Avg Packets</b>: %{y:.2f}<br>' +
            '<b>Avg Duration</b>: %{z:.2f}<extra></extra>'
        )])

        fig_3d_bar.update_layout(
            title='3D Protocol Characteristics Analysis',
            scene=dict(
                xaxis_title='Average Forward Bytes',
                yaxis_title='Average Forward Packets',
                zaxis_title='Average Flow Duration',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_bar.show()

        # 4. 3D Anomaly Visualization
        pca = PCA(n_components=3)
        X_pca = pca.fit_transform(X)

        fig_3d_anomaly = go.Figure(data=[go.Scatter3d(
            x=X_pca[:, 0],
            y=X_pca[:, 1],
            z=X_pca[:, 2],
            mode='markers',
            marker=dict(
                size=5,
                color=iso_forest_scores,
                colorscale=[[0, 'red'], [1, 'blue']],
                opacity=0.8,
                colorbar=dict(
                    title='Anomaly Score',
                    ticktext=['Anomaly', 'Normal'],
                    tickvals=[-1, 1]
                )
            ),
            hovertemplate=
            '<b>PC1</b>: %{x:.2f}<br>' +
            '<b>PC2</b>: %{y:.2f}<br>' +
            '<b>PC3</b>: %{z:.2f}<br>' +
            '<b>Status</b>: %{marker.color}<extra></extra>'
        )])

        fig_3d_anomaly.update_layout(
            title='3D Anomaly Detection Visualization (PCA)',
            scene=dict(
                xaxis_title='Principal Component 1',
                yaxis_title='Principal Component 2',
                zaxis_title='Principal Component 3',
                camera=dict(
                    up=dict(x=0, y=0, z=1),
                    center=dict(x=0, y=0, z=0),
                    eye=dict(x=1.5, y=1.5, z=1.5)
                )
            ),
            width=1000,
            height=800
        )
        fig_3d_anomaly.show()

    except Exception as e:
        print(f"Error in 3D visualization: {e}")

def train_anomaly_detectors(X):
    """Train multiple anomaly detection models"""
    try:
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        iso_forest_scores = iso_forest.fit_predict(X)

        # One-Class SVM
        ocsvm = OneClassSVM(kernel='rbf', nu=0.1)
        ocsvm_scores = ocsvm.fit_predict(X)

        return iso_forest_scores, ocsvm_scores
    except Exception as e:
        print(f"Error in training anomaly detectors: {e}")
        return None, None

def train_rl_detector(X_train, y_train, epochs=10):
    """Train RL-based anomaly detector"""
    try:
        state_size = X_train.shape[1]
        action_size = 2  # Normal or Anomaly
        agent = DQNAgent(state_size, action_size)

        batch_size = 32
        rewards_history = []

        for epoch in range(epochs):
            total_reward = 0
            for i in range(0, len(X_train), batch_size):
                state = X_train[i:i+batch_size]
                if len(state) < batch_size:
                    break

                state = np.reshape(state[0], [1, state_size])
                action = agent.act(state)

                # Calculate reward based on correct prediction
                true_label = y_train[i]
                reward = 1 if action == true_label else -1

                next_state = X_train[i+1:i+batch_size+1]
                if len(next_state) < batch_size:
                    break
                next_state = np.reshape(next_state[0], [1, state_size])

                done = i + batch_size >= len(X_train)
                agent.train(state, action, reward, next_state, done)
                total_reward += reward

            rewards_history.append(total_reward)
            if epoch % 10 == 0:
                print(f"Epoch: {epoch}, Total Reward: {total_reward}")

        return agent, rewards_history
    except Exception as e:
        print(f"Error in training RL detector: {e}")
        return None, None

def analyze_network_traffic(file_paths):
    """Main analysis function with error handling and enhanced visualizations"""
    try:
        # Load and combine data
        dataframes = []
        for path in file_paths:
            try:
                df = pd.read_csv(path)
                dataframes.append(df)
            except Exception as e:
                print(f"Error loading {path}: {e}")
                continue

        if not dataframes:
            print("No data loaded. Please check file paths.")
            return

        combined_df = pd.concat(dataframes, ignore_index=True)

        # Calculate flow duration
        combined_df['flow_duration'] = combined_df['fwd_mean_iat'] * combined_df['fwd_num_pkts']
        combined_df['flow_duration'] = combined_df['flow_duration'].replace(0, 1)  # Avoid division by zero

        # Create 3D visualizations
        create_3d_traffic_plots(combined_df)

        # Original Protocol Distribution
        try:
            protocol_map = {
                6: '6: TCP',
                17: '17: UDP'
            }

            # Replace the numbers in the 'proto' column with their mapped labels
            combined_df['proto'] = combined_df['proto'].map(protocol_map).fillna(combined_df['proto'])

            # Create the pie chart
            fig_proto = go.Figure(data=[go.Pie(
                labels=combined_df['proto'].value_counts().index,
                values=combined_df['proto'].value_counts().values,
                textinfo='label+percent',
                marker=dict(colors=px.colors.qualitative.Set3),
                hole=0.3
            )])

            # Update layout for visualization
            fig_proto.update_layout(
                title='Network Protocol Distribution Analysis',
                annotations=[dict(text="Protocol Types", x=0.5, y=0.5, font_size=14, showarrow=False)],
                height=600,
                width=800
            )
            fig_proto.show()
        except Exception as e:
            print(f"Error in protocol distribution visualization: {e}")

        # Flow Metrics
        try:
            fig_basic = create_subplot_figure(2, 2, [
                'Network Bytes Distribution (log scale)',
                'Packet Count Distribution (log scale)',
                'Flow Duration Analysis (log scale)',
                'Inter-Arrival Time Distribution (log scale)'
            ])

            if fig_basic:
                for i, (data, name) in enumerate([
                    (np.log10(combined_df['fwd_num_bytes'] + 1), 'Forward Bytes'),
                    (np.log10(combined_df['fwd_num_pkts'] + 1), 'Forward Packets'),
                    (np.log10(combined_df['flow_duration'] + 1), 'Flow Duration'),
                    (np.log10(combined_df['fwd_mean_iat'] + 1), 'Forward IAT')
                ]):
                    fig_basic.add_trace(
                        go.Histogram(
                            x=data,
                            name=name,
                            nbinsx=50,
                            marker_color=px.colors.qualitative.Set2[i]
                        ),
                        row=(i // 2) + 1,
                        col=(i % 2) + 1
                    )
                    fig_basic.update_xaxes(title_text='Log10 Scale', row=(i // 2) + 1, col=(i % 2) + 1)
                    fig_basic.update_yaxes(title_text='Count', row=(i // 2) + 1, col=(i % 2) + 1)

                fig_basic.show()
        except Exception as e:
            print(f"Error in flow metrics visualization: {e}")

        # Prepare data for modeling
        X, numeric_cols = prepare_data(combined_df)
        if X is None or numeric_cols is None:
            return

        # Train traditional anomaly detectors
        iso_forest_scores, ocsvm_scores = train_anomaly_detectors(X)
        if iso_forest_scores is None or ocsvm_scores is None:
            return

        # Prepare data for RL
        y = (iso_forest_scores == -1).astype(int)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train RL detector
        rl_agent, rewards_history = train_rl_detector(X_train, y_train, epochs=30)
        if rl_agent is None or rewards_history is None:
            return

        # Generate RL predictions
        rl_predictions = []
        try:
            for i in range(len(X_test)):
                state = np.reshape(X_test[i], [1, X_test.shape[1]])
                rl_predictions.append(rl_agent.act(state))
        except Exception as e:
            print(f"Error generating RL predictions: {e}")
            return

        # Model Comparison Analysis
        try:
            fig_comparison = create_subplot_figure(2, 2, [
                'Detection Methods Comparison',
                'RL Training Rewards',
                'Model Performance Metrics',
                'ROC Curves Comparison'
            ])

            if fig_comparison:
                # Detection Methods Comparison
                methods = ['Isolation Forest', 'One-Class SVM', 'RL-Based']
                scores = [
                    accuracy_score(y_test, iso_forest_scores[-len(y_test):] == -1),
                    accuracy_score(y_test, ocsvm_scores[-len(y_test):] == -1),
                    accuracy_score(y_test, rl_predictions)
                ]

                fig_comparison.add_trace(
                    go.Bar(
                        x=methods,
                        y=scores,
                        marker_color=px.colors.qualitative.Set1
                    ),
                    row=1, col=1
                )

                # RL Training Rewards
                fig_comparison.add_trace(
                    go.Scatter(
                        y=rewards_history,
                        mode='lines',
                        name='RL Rewards',
                        line=dict(color=px.colors.qualitative.Set1[2])
                    ),
                    row=1, col=2
                )

                # Model Performance Metrics
                metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
                rl_metrics = [
                    accuracy_score(y_test, rl_predictions),
                    precision_score(y_test, rl_predictions),
                    recall_score(y_test, rl_predictions),
                    f1_score(y_test, rl_predictions)
                ]

                fig_comparison.add_trace(
                    go.Bar(
                        x=metrics,
                        y=rl_metrics,
                        name='RL Model Metrics',
                        marker_color=px.colors.qualitative.Set1[2]
                    ),
                    row=2, col=1
                )

                # ROC Curves
                for model_name, y_pred in [
                    ('Isolation Forest', iso_forest_scores[-len(y_test):] == -1),
                    ('One-Class SVM', ocsvm_scores[-len(y_test):] == -1),
                    ('RL-Based', rl_predictions)
                ]:
                    fpr, tpr, _ = roc_curve(y_test, y_pred)
                    auc_score = auc(fpr, tpr)
                    fig_comparison.add_trace(
                        go.Scatter(
                            x=fpr,
                            y=tpr,
                            name=f'{model_name} (AUC={auc_score:.3f})',
                            mode='lines'
                        ),
                        row=2, col=2
                    )

                fig_comparison.update_xaxes(title_text='False Positive Rate', row=2, col=2)
                fig_comparison.update_yaxes(title_text='True Positive Rate', row=2, col=2)
                fig_comparison.show()

        except Exception as e:
            print(f"Error in model comparison visualization: {e}")

    except Exception as e:
        print(f"Error in network traffic analysis: {e}")

def main():
    """Main function with error handling"""
    try:
        file_paths = [
            '/content/new_biflow_Thursday-WorkingHours_Wev_XSS.csv',
            '/content/new_biflow_Thursday-WorkingHours_CoolDisk.csv',
            '/content/new_biflow_Thursday-WorkingHours_Dropbox1.csv',
            '/content/new_biflow_Thursday-WorkingHours_Dropbox2.csv',
            '/content/new_biflow_Thursday-WorkingHours_Web_BF.csv'
        ]

        print("Starting network traffic analysis...")
        analyze_network_traffic(file_paths)
        print("Analysis completed successfully!")

    except Exception as e:
        print(f"Error in main function: {e}")

if __name__ == "__main__":
    main()